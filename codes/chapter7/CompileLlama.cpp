# export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu/:$LD_LIBRARY_PATH
# CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python
